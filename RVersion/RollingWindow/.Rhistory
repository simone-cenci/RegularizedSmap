coeff <- solve(t(Xx) %*% WWs %*% Xx + lambda*nrow(Xx)*diag(1,dimension + 1)) %*% t(Xx) %*%(WWs %*% y)
coeff <- t(coeff)
return(coeff)
}
for (ipred in 1:length(pred)){
libs = lib[-pred[ipred]]
q <- matrix(as.numeric(block[pred[ipred],2:dim(block)[2]]),
ncol=Edim, nrow=length(libs), byrow = T)
distances <- sqrt(rowSums((block[libs,2:dim(block)[2]] - q)^2))
Krnl = match.fun(Regression.Kernel)
Ws = Krnl(distances, theta)
fit <- lm_regularized(as.matrix(block[libs,1:Edim]),
as.matrix(block[libs,(Edim+1):dim(block)[2]]),Ws, lambda, Edim)
Js[[ipred]] <- fit[,2:(Edim+1)]
c0s[[ipred]] <- fit[,1]
}
#### Now compute the all time series of Jacobian coefficients
Jacobian.TS = ridge_fit(d.training,BestModel$BestTH[1], BestModel$LM[1], alpha, Regression.Kernel)
alpha
BestModel$LM[1]
BestModel$LM
#### Now compute the all time series of Jacobian coefficients
Jacobian.TS = ridge_fit(d.training,BestModel$BestTH[1], BestModel$BestLM[1], alpha, Regression.Kernel)
View(Jacobian.TS)
source('~/Desktop/RollingCV/main.r')
other.parameters[idx,1]
source('~/Desktop/RollingCV/main.r')
source('~/Desktop/RollingCV/main.r')
rm(list = ls())
suppressMessages(library(Matrix))
suppressMessages(library(quantreg))
suppressMessages(library(parallel))
suppressMessages(library(compiler))
suppressMessages(library(lars))
suppressMessages(library(elasticnet))
options(warn=-1)
#####################################################################################
source('Auxiliar.r')
source('elastic_net_fit.r')
source('moving_window.r')
source('KernelFunctions.r')
source('OutOfSample.r')
source('Interactions.R')
source('ridge.r')
Kernel.Options = c('Exponential.Kernel', 'Epanechnikov.Kernel', 'TriCubic.Kernel')
###################################
logspace <- function(d1, d2, n) exp(log(10)*seq(d1, d2, length.out=n))
####################################
test.error.rho = test.error.rmse = c()
######
options.models = c('inputFiles/PredatorPrey.txt', 'inputFiles/RPS.txt',
'inputFiles/Chaotic_LV.txt')
options.jacobian = c('inputFiles/jacobian_predator.txt', 'inputFiles/jacobian_rps.txt',
'inputFiles/jacobian_chaos.txt')
### The regression kernel should be chosen from 'model_selection.r'
choice = 3
FileName = options.models[choice]
JacobianName = options.jacobian[choice]
######
length.training = 100
length.testing = 30
######
### Read Time series
d = as.matrix(read.table(FileName))
### Use all the species
TargetList = LETTERS[1:ncol(d)]
colnames(d) = TargetList
######################
dfdx = expand.grid(TargetList, TargetList)
######################
t.min = floor(runif(1,1, nrow(d) - length.training - length.testing-1))
#### Random Chunk of length:
length_of_interval = length.training + length.testing
t.max = t.min + length_of_interval - 1
interval = t.min:t.max
interval_training = 1:length.training
interval_testing = (length.training + 1):length_of_interval
#### Subset the chunk
d_intact = d[interval,]
d.training = d_intact[interval_training, ]
#### Make noise if you want
ObservationalNoise = 0
if(ObservationalNoise == 1){
d.training = d.training + matrix(rnorm(length(d.training), 0,mean(d.training)*0.05),
nrow(d.training), ncol(d.training))
cat('Added observational noise\n')
}
### Preserve the training set to standardize the test set
d.train.to.test.set = d.training
d.training = Standardizza(d.training)
Regression.Kernel
time.series = d.training
theta = 0.1
lambda = 0.1
alp = 0.9
Regression.Kernel = 'Exponential.Kernel'
Edim <- ncol(time.series)
block <- cbind(time.series[2:dim(time.series)[1],],time.series[1:(dim(time.series)[1]-1),])
block <- as.data.frame(apply(block, 2, function(x) (x-mean(x))/sd(x)))
lib <- 1:dim(block)[1]
pred <- 1:dim(block)[1]
lm_regularized <- function(y, x, ws, lambda, dimension, subset = seq_along(y)){
#########################################################
WWs = diag(ws)
Xx = as.matrix(x)
Xx = cbind(1, Xx)
coeff <- solve(t(Xx) %*% WWs %*% Xx + lambda*nrow(Xx)*diag(1,dimension + 1)) %*% t(Xx) %*%(WWs %*% y)
coeff <- t(coeff)
return(coeff)
}
ipred = length(pred)
libs = lib[-pred[ipred]]
q <- matrix(as.numeric(block[pred[ipred],2:dim(block)[2]]),
ncol=Edim, nrow=length(libs), byrow = T)
distances <- sqrt(rowSums((block[libs,2:dim(block)[2]] - q)^2))
Krnl = match.fun(Regression.Kernel)
Ws = Krnl(distances, theta)
fit <- lm_regularized(as.matrix(block[libs,1:Edim]),
as.matrix(block[libs,(Edim+1):dim(block)[2]]),Ws, lambda, Edim)
coeff <- fit
colnames(coeff) <- c('c0', letters[1:Edim])
q <- matrix(as.numeric(block[pred[ipred],(Edim+1):dim(block)[2]]),
ncol=Edim, nrow=length(libs), byrow = T)
distances <- sqrt(rowSums((block[libs,(Edim+1):dim(block)[2]] - q)^2))
Krnl = match.fun(Regression.Kernel)
Ws = Krnl(distances, theta)
fit <- lm_regularized(as.matrix(block[libs,1:Edim]),
as.matrix(block[libs,(Edim+1):dim(block)[2]]),Ws, lambda, Edim)
coeff <- fit
View(coeff)
source('~/Dropbox (MIT)/ProvaToWorkOn/main.r')
source('~/Dropbox (MIT)/ProvaToWorkOn/main.r')
source('~/Dropbox (MIT)/ProvaToWorkOn/main.r')
source('~/Dropbox (MIT)/ProvaToWorkOn/main.r')
######
Lavoratori = detectCores()-1
cl <- makeCluster(Lavoratori, type = "FORK")
BestModel = MV.CV(cl, d.training, parameters_on_grid, best.alpha,
RegressionType, Regression.Kernel, TargetList)
dim(other.parameters)
dim(other.parameters)[1]
source('~/Dropbox (MIT)/ProvaToWorkOn/main.r')
#### Now compute the all time series of Jacobian coefficients
Jacobian.TS = ridge_fit(d.training,best.theta, best.lambda, alpha, Regression.Kernel)
source('~/Dropbox (MIT)/ProvaToWorkOn/main.r')
source('~/Dropbox (MIT)/ProvaToWorkOn/main.r')
source('~/Dropbox (MIT)/ProvaToWorkOn/main.r')
median(correlation)
median(inference.rho)
hist(correlation)
hist(inference.rho)
hist(RMSE)
median(correlation)
median(inference.rho)
median(correlation)
median(inference.rho)
source('~/Dropbox (MIT)/ProvaToWorkOn/main.r')
source('~/Dropbox (MIT)/ProvaToWorkOn/main.r')
source('~/Dropbox (MIT)/ProvaToWorkOn/main.r')
source('~/Dropbox (MIT)/ProvaToWorkOn/main.r')
source('~/Dropbox (MIT)/ProvaToWorkOn/main.r')
source('~/Dropbox (MIT)/ProvaToWorkOn/main.r')
source('~/Dropbox (MIT)/ProvaToWorkOn/main.r')
source('~/Dropbox (MIT)/ProvaToWorkOn/main.r')
source('~/Dropbox (MIT)/ProvaToWorkOn/main.r')
source('~/Dropbox (MIT)/ProvaToWorkOn/main.r')
source('~/Dropbox (MIT)/ProvaToWorkOn/main.r')
source('~/Dropbox (MIT)/ProvaToWorkOn/main.r')
source('~/Dropbox (MIT)/ProvaToWorkOn/main.r')
source('~/Dropbox (MIT)/ProvaToWorkOn/main.r')
source('~/Dropbox (MIT)/ProvaToWorkOn/main.r')
source('~/Dropbox (MIT)/ProvaToWorkOn/main.r')
source('~/Dropbox (MIT)/ProvaToWorkOn/main.r')
source('~/Dropbox (MIT)/ProvaToWorkOn/main.r')
View(val_err)
source('~/Dropbox (MIT)/ProvaToWorkOn/main.r')
source('~/Dropbox (MIT)/ProvaToWorkOn/main.r')
View(val_err)
source('~/Dropbox (MIT)/ProvaToWorkOn/main.r')
source('~/Dropbox (MIT)/ProvaToWorkOn/main.r')
median(correlation)
median(correlation)
median(inference.rho)
hist(correlation)
source('~/Desktop/ProvaToWorkON/main.r')
install.packages('tictoc')
source('~/Desktop/ProvaToWorkON/main.r')
update.elnet <- function(time.series, targ_col, theta, lambda,alp, Regression.Kernel)
{
Edim <- ncol(time.series)
block <- cbind(time.series[2:dim(time.series)[1],],time.series[1:(dim(time.series)[1]-1),])
block <- as.data.frame(apply(block, 2, function(x) (x-mean(x))/sd(x)))
lib <- 1:dim(block)[1]
pred <- 1:dim(block)[1]
coeff <- array(0,dim=c(Edim+1,Edim + 1))
colnames(coeff) <- c('c0',letters[1:Edim])
ipred = length(pred)
libs = lib[-pred[ipred]]
q <- matrix(as.numeric(block[pred[ipred],2:dim(block)[2]]),
ncol=Edim, nrow=length(libs), byrow = T)
distances <- sqrt(rowSums((block[libs,2:dim(block)[2]] - q)^2))
### Kernel
Krnl = match.fun(Regression.Kernel)
Ws = sqrt(Krnl(distances, theta))
############ Fit function
x = as.matrix(block[libs,(Edim+1):dim(block)[2]])
y = as.matrix(block[libs,1:Edim])
x = as.matrix(Ws * cbind(1, x))
y = as.matrix(Ws * y)
fit <- enet(x, y, lambda = lambda, normalize = TRUE, intercept = FALSE)
coeff <- predict(fit, s = alp, type="coefficients", mode="fraction")$coefficients
return(coeff)
}
update.ridge <- function(time.series, theta, lambda, alp, Regression.Kernel)
{
Edim <- ncol(time.series)
block <- cbind(time.series[2:dim(time.series)[1],],time.series[1:(dim(time.series)[1]-1),])
block <- as.data.frame(apply(block, 2, function(x) (x-mean(x))/sd(x)))
lib <- 1:dim(block)[1]
pred <- 1:dim(block)[1]
lm_regularized <- function(y, x, ws, lambda, dimension, subset = seq_along(y)){
#########################################################
WWs = diag(ws)
Xx = as.matrix(x)
Xx = cbind(1, Xx)
coeff <- solve(t(Xx) %*% WWs %*% Xx + lambda*nrow(Xx)*diag(1,dimension + 1)) %*% t(Xx) %*%(WWs %*% y)
return(t(coeff))
}
ipred = length(pred)
libs = lib[-pred[ipred]]
q <- matrix(as.numeric(block[pred[ipred],(Edim+1):dim(block)[2]]),
ncol=Edim, nrow=length(libs), byrow = T)
distances <- sqrt(rowSums((block[libs,(Edim+1):dim(block)[2]] - q)^2))
Krnl = match.fun(Regression.Kernel)
Ws = Krnl(distances, theta)
fit_ <- lm_regularized(as.matrix(block[libs,1:Edim]),
as.matrix(block[libs,(Edim+1):dim(block)[2]]),Ws, lambda, Edim)
colnames(fit_) <- c('c0', letters[1:Edim])
return(list(J = fit_[,2:(Edim+1)], c0 = fit_[,1]))
}
#### Forecast
forecast.function <- function(th, lm, alpha, ts_training, num_points,
RegressionType, Regression.Kernel){
out_of_samp = c()
### Take the last point in the training set
new_point = ts_training[nrow(ts_training), ]
updated.fit = update.ridge(ts_training, th, lm, alpha, Regression.Kernel)
for(j in 1:num_points){
### Predict the first point in the training set and then allthe others
new_point = Testing(updated.fit$J, updated.fit$c0, new_point)
out_of_samp = rbind(out_of_samp, t(new_point))
ts_training = Add_to_TS(ts_training, t(new_point))
updated.fit = update.ridge(ts_training, th, lm, alpha, Regression.Kernel)
}
return(out_of_samp)
}
mv <- function(s, X, grid, RegressionType, trg.list)
{
#### Rolling window cross validation
#### -----------0***  |
#### ------------0*** |
#### -------------0***|
#### -------------0****
#### Look carefully at the way you divide training and validation
length.validating = 10
repetitions = 10
validation.error = rep(0,repetitions+1)
for(i in 0:repetitions)
{
x.training = X[1:(nrow(X) - length.validating - repetitions + i),]
x.validating = X[(nrow(X) - length.validating - repetitions+i+1):(nrow(X)- length.validating - repetitions+i+length.validating),]
alpha = grid[s,3]
Regression.Kernel = Kernel.Options[grid[s,4]]
out.of.samp = forecast.function(grid[s,1], grid[s,2], alpha,x.training,
nrow(x.validating),
RegressionType, Regression.Kernel, trg.list)
validation.error[i] = sqrt(mean((out.of.samp - x.validating)**2))
}
return(list(bndwth = grid[s,1], lmb = grid[s,2], al = grid[s,3], krn = grid[s,4], rmse = mean(validation.error)))
}
MV.CV <- function(cl, data.df, grid, RegressionType, trg.list){
S_target <- parLapply(cl, 1:nrow(grid), mv, data.df, grid, RegressionType, trg.list)
error.mat = cbind(unlist(S_target)[attr(unlist(S_target),"names") == "bndwth"],
unlist(S_target)[attr(unlist(S_target),"names") == "lmb"],
unlist(S_target)[attr(unlist(S_target),"names") == "al"],
unlist(S_target)[attr(unlist(S_target),"names") == "krn"],
unlist(S_target)[attr(unlist(S_target),"names") == "rmse"])
rownames(error.mat) = c()
error.mat[,5] = round(error.mat[,5],3)
error.mat = error.mat[order(error.mat[,5]), ]
idx = which(error.mat[,5] == min(error.mat[,5]))
idx.th = which(grid[idx,1] == min(grid[idx,1]))
idx = idx[min(idx.th)]
return(list(BestTH = rep(error.mat[idx,1], ncol(data.df)),
BestLM = rep(error.mat[idx,2], ncol(data.df)),
BestAL = error.mat[idx,3],
BestKRN = error.mat[idx,4],
min.val.err = error.mat[idx,5],
val.err = error.mat))
}
mv.training <- function(cl, X, grid, Reg.type, Krn.Options)
{
### X = training data
trg.list = colnames(X)
dfdx = expand.grid(trg.list, trg.list)
######################
fit_ = MV.CV(cl, X, grid, Reg.type, trg.list)
best.theta = fit_$BestTH[1]
best.lambda = fit_$BestLM[1]
best.alpha = fit_$BestAL
Regression.Kernel = Krn.Options[fit_$BestKRN]
#### Make coefficients
Jacobian.TS = BuildJacobians(X, trg.list, best.theta, best.lambda,
best.alpha, Reg.type, Regression.Kernel)$J
return(list(best.theta = fit_$BestTH[1], best.lambda = fit_$BestLM[1],
best.alpha = fit_$BestAL, Regression.Kernel = Kernel.Options[fit_$BestKRN],
jacobiano = Jacobian.TS))
}
#### Run rolling window cross validation
output.training = mv.training(cl, d.training, parameters_on_grid, RegressionType, Kernel.Options)
source('~/Desktop/ProvaToWorkON/main.r')
source('~/Desktop/ProvaToWorkON/main.r')
source('~/Desktop/ProvaToWorkON/main.r')
View(parameters_on_grid)
X = d.training
grid = parameters_on_grid
### X = training data
fit_ = MV.CV(cl, X, grid)
source('~/Desktop/ProvaToWorkON/main.r')
source('~/Desktop/ProvaToWorkON/main.r')
source('~/Desktop/ProvaToWorkON/main.r')
source('~/Desktop/ProvaToWorkON/main.r')
source('~/Desktop/ProvaToWorkON/main.r')
source('~/Desktop/ProvaToWorkON/main.r')
source('~/Desktop/ProvaToWorkON/main.r')
source('~/Desktop/ProvaToWorkON/main.r')
source('~/Desktop/ProvaToWorkON/main.r')
View(output.training)
View(output.training$validation.error)
source('~/Desktop/ProvaToWorkON/main.r')
source('~/Desktop/ProvaToWorkON/main.r')
View(output.training)
source('~/Desktop/ProvaToWorkON/main.r')
source('~/Desktop/ProvaToWorkON/main.r')
source('~/Desktop/ProvaToWorkON/main.r')
source('~/Desktop/ProvaToWorkON/main.r')
source('~/Desktop/ProvaToWorkON/main.r')
View(output.training)
source('~/Desktop/ProvaToWorkON/main.r')
source('~/Desktop/ProvaToWorkON/main.r')
source('~/Desktop/ProvaToWorkON/main.r')
View(output.training)
View(output.training$validation.error)
median(correlation)
median(inference.rho)
median(correlation)
median(inference.rho)
median(correlation)
median(inference.rho)
source('~/Desktop/Code/main.r')
source('~/Desktop/Code/main.r')
source('~/Desktop/Code/main.r')
stopCluster(cl)
source('~/Desktop/Code/main.r')
source('~/Dropbox (MIT)/RVersion/RollingWindow/main.r')
install.packages('quantreg')
source('~/Dropbox (MIT)/RVersion/RollingWindow/main.r')
install.packages('lars')
install.packages('tictoc')
install.packages('elasticnet')
source('~/Dropbox (MIT)/RVersion/RollingWindow/main.r')
source('Functions/Interactions.R')
rho = jacobiano_analitico(JacobianName,
Jacobian.TS,
ncol(d), t.min, t.min+length.training-1)
rho = jacobiano_analitico(JacobianName,
Jacobian.TS,
ncol(d.training), t.min, t.min+length.training-1)
jacobiano_analitico <- function(nome, Inferred, num_species, origin, end){
### Inferred is the infer jacobian coefficients
### example jacobiano_analitico(BestCoefficients.elnet$J, 2, t.min, t.min+length.training-2)
nomi = list()
nomi$row = nomi$col = LETTERS[1:num_species]
all.data = as.matrix(read.table(nome, header = F))
### Nota importante: e fondamentale che transponi la matrice per come li stampi da python
J = lapply(origin:end, function(x, X) t(matrix(X[x,], num_species,
num_species, dimnames = nomi)), all.data)
result = Models.coefficient.correlation(Inferred, J, dfdx)
tr.true = unlist(lapply(2:(length(J)), function(x, X) sum(diag(X[[x]])), J))
tr.inferred = unlist(lapply(1:length(Inferred), function(x, X) sum(diag(X[[x]])), Inferred))
tr.inferred = tr.inferred[!is.na(tr.inferred)]
trace.corr = cor(scale(tr.true), scale(tr.inferred))
return(list(correlation.matrix = result$mat.cor, rmse.matrix = result$mat.rmse,
analytical.jacobian = J, trace.correlation = trace.corr,
true.trace = tr.true, inferred.trace = tr.inferred))
}
Models.coefficient.correlation <- function(X, Y){
### input X,Y = Best series of jacobian coefficients
###       dfdx = a dataframe with all the combination of names of species (Jacobian entry)
### output: A matrix of which each entry tells me the correlation coefficient between
###         matrix X and matrix Y
CorMat = rmse.mat = c()
dfdx = expand.grid(LETTER[1:nrow(X[[1]])], LETTER[1:nrow(X[[1]])])
for(k in 1:nrow(dfdx)){
X_ij = unlist(lapply(1:(length(X)), function(i, M) M[[i]][dfdx[k,1],dfdx[k,2]], X))
Y_ij = unlist(lapply(1:(length(Y)-1), function(i, M) M[[i]][dfdx[k,1],dfdx[k,2]], Y))
CorMat = c(CorMat, cor(X_ij, Y_ij))
rmse.mat = c(rmse.mat, sqrt(mean((X_ij - Y_ij)^2)))
}
return(list( mat.cor = matrix(CorMat, nrow(X[[1]]), nrow(X[[1]])),
mat.rmse = matrix(rmse.mat,nrow(X[[1]]), nrow(X[[1]]))))
}
jacobiano_analitico <- function(nome, Inferred, num_species, origin, end){
### Inferred is the infer jacobian coefficients
### example jacobiano_analitico(BestCoefficients.elnet$J, 2, t.min, t.min+length.training-2)
nomi = list()
nomi$row = nomi$col = LETTERS[1:num_species]
all.data = as.matrix(read.table(nome, header = F))
### Nota importante: e fondamentale che transponi la matrice per come li stampi da python
J = lapply(origin:end, function(x, X) t(matrix(X[x,], num_species,
num_species, dimnames = nomi)), all.data)
result = Models.coefficient.correlation(Inferred, J)
tr.true = unlist(lapply(2:(length(J)), function(x, X) sum(diag(X[[x]])), J))
tr.inferred = unlist(lapply(1:length(Inferred), function(x, X) sum(diag(X[[x]])), Inferred))
tr.inferred = tr.inferred[!is.na(tr.inferred)]
trace.corr = cor(scale(tr.true), scale(tr.inferred))
return(list(correlation.matrix = result$mat.cor, rmse.matrix = result$mat.rmse,
analytical.jacobian = J, trace.correlation = trace.corr,
true.trace = tr.true, inferred.trace = tr.inferred))
}
Models.coefficient.correlation <- function(X, Y){
### input X,Y = Best series of jacobian coefficients
###       dfdx = a dataframe with all the combination of names of species (Jacobian entry)
### output: A matrix of which each entry tells me the correlation coefficient between
###         matrix X and matrix Y
CorMat = rmse.mat = c()
dfdx = expand.grid(LETTER[1:nrow(X[[1]])], LETTER[1:nrow(X[[1]])])
for(k in 1:nrow(dfdx)){
X_ij = unlist(lapply(1:(length(X)), function(i, M) M[[i]][dfdx[k,1],dfdx[k,2]], X))
Y_ij = unlist(lapply(1:(length(Y)-1), function(i, M) M[[i]][dfdx[k,1],dfdx[k,2]], Y))
CorMat = c(CorMat, cor(X_ij, Y_ij))
rmse.mat = c(rmse.mat, sqrt(mean((X_ij - Y_ij)^2)))
}
return(list( mat.cor = matrix(CorMat, nrow(X[[1]]), nrow(X[[1]])),
mat.rmse = matrix(rmse.mat,nrow(X[[1]]), nrow(X[[1]]))))
}
rho = jacobiano_analitico(JacobianName,
Jacobian.TS,
ncol(d.training), t.min, t.min+length.training-1)
jacobiano_analitico <- function(nome, Inferred, num_species, origin, end){
### Inferred is the infer jacobian coefficients
### example jacobiano_analitico(BestCoefficients.elnet$J, 2, t.min, t.min+length.training-2)
nomi = list()
nomi$row = nomi$col = LETTERS[1:num_species]
all.data = as.matrix(read.table(nome, header = F))
### Nota importante: e fondamentale che transponi la matrice per come li stampi da python
J = lapply(origin:end, function(x, X) t(matrix(X[x,], num_species,
num_species, dimnames = nomi)), all.data)
result = Models.coefficient.correlation(Inferred, J)
tr.true = unlist(lapply(2:(length(J)), function(x, X) sum(diag(X[[x]])), J))
tr.inferred = unlist(lapply(1:length(Inferred), function(x, X) sum(diag(X[[x]])), Inferred))
tr.inferred = tr.inferred[!is.na(tr.inferred)]
trace.corr = cor(scale(tr.true), scale(tr.inferred))
return(list(correlation.matrix = result$mat.cor, rmse.matrix = result$mat.rmse,
analytical.jacobian = J, trace.correlation = trace.corr,
true.trace = tr.true, inferred.trace = tr.inferred))
}
Models.coefficient.correlation <- function(X, Y){
### input X,Y = Best series of jacobian coefficients
###       dfdx = a dataframe with all the combination of names of species (Jacobian entry)
### output: A matrix of which each entry tells me the correlation coefficient between
###         matrix X and matrix Y
CorMat = rmse.mat = c()
dfdx = expand.grid(LETTERS[1:nrow(X[[1]])], LETTERS[1:nrow(X[[1]])])
for(k in 1:nrow(dfdx)){
X_ij = unlist(lapply(1:(length(X)), function(i, M) M[[i]][dfdx[k,1],dfdx[k,2]], X))
Y_ij = unlist(lapply(1:(length(Y)-1), function(i, M) M[[i]][dfdx[k,1],dfdx[k,2]], Y))
CorMat = c(CorMat, cor(X_ij, Y_ij))
rmse.mat = c(rmse.mat, sqrt(mean((X_ij - Y_ij)^2)))
}
return(list( mat.cor = matrix(CorMat, nrow(X[[1]]), nrow(X[[1]])),
mat.rmse = matrix(rmse.mat,nrow(X[[1]]), nrow(X[[1]]))))
}
rho = jacobiano_analitico(JacobianName,
Jacobian.TS,
ncol(d.training), t.min, t.min+length.training-1)
matrix.inference.rho = rho$correlation.matrix
####
cat('Trace inference:', rho$trace.correlation, '\n')
#### Predict
prd = forecast.function(rep(output.training$best.theta,ncol(d.training)),
rep(output.training$best.lambda,ncol(d.training)),
d.training, length.testing,
output.training$Regression.Kernel)
d.testing =  d[(t.min + length.training):(t.min + length.training + length.testing - 1), ]
d.testing = Standardizza.test(d.testing, d.train.to.test.set)
correlation = mean(unlist(lapply(1:ncol(d.testing), function(x, X, Y) cor(X[,x], Y[,x]), prd, d.testing)))
d.testing =  as.matrix(read.table(input.file))[(t.min + length.training):(t.min + length.training + length.testing - 1),]
d.testing = Standardizza.test(d.testing, d.train.to.test.set)
correlation = mean(unlist(lapply(1:ncol(d.testing), function(x, X, Y) cor(X[,x], Y[,x]), prd, d.testing)))
cat('correlation:', correlation, '\n')
toc()
#### Make some plot
## VCR
plot(scale(rho$true.trace), type = 'l', lwd = 3)
lines(scale(rho$inferred.trace), lwd = 3, col = 'red')
## Forecast
idx = 2
plot(d.testing[,idx], type = 'l', lwd = 3)
lines(prd[,idx], col = 'red', lwd = 3)
source('~/Dropbox (MIT)/RVersion/RollingWindow/main.r')
matrix.inference.rho
