###################################
#cat('Model:',  FileName, '\nKernel:', Regression.Kernel,'\n')
###################################
logspace <- function(d1, d2, n) exp(log(10)*seq(d1, d2, length.out=n))
std_err <- function(x) sd(x)/sqrt(length(x))
############# Parameters for cross validation
lambda = logspace(min.lambda[choice],0,15)
tht = logspace(-1.5,1.2,15)
parameters_on_grid = expand.grid(tht, lambda)
##### length of training and test set
length.training = training.size[choice]
length.testing = 30
### Read Time series
d = ReadTimeSeries(FileName)
### Use all the species
Embedding = LETTERS[1:ncol(d)]
TargetList = Embedding
######################
dfdx = expand.grid(TargetList, TargetList)
######################
d = d[, Embedding]
result = c()
t.min = 1
#### If you want to measure performance across random starting points initialized randomly
#t.min = floor(runif(1,1, nrow(d) - length.training - length.testing-1))
#### Random Chunk of length:
length_of_interval = length.training + length.testing
t.max = t.min + length_of_interval - 1
interval = t.min:t.max
interval_training = 1:length.training
interval_testing = (length.training + 1):length_of_interval
#### Subset the chunk
d_intact = d[interval,]
d.training = d_intact[interval_training, ]
#### Make noise if you want
ObservationalNoise = c('no_noise', 'normal', 'gamma', 'exponential')
ObservationalNoise = ObservationalNoise[noise.type]
a = d.training
plot(a[,3], type = 'l')
rumore = c(0.05,0.05,0.05,0.1,0.1,0.05,0.05,0.05)
d.training = d.training + matrix(rnorm(length(d.training),
0,sd(d.training)*rumore[model]),
nrow(d.training), ncol(d.training))
#a = d.training
plot(a[,3], type = 'l')
lines(d.training[,3], col = 'red')
d.training = a
shape = c(30, .8, 60, 10, 10, 10, 2, 35)
d.training = d.training + matrix(rgamma(length(d.training), 1, shape[model]),
nrow(d.training), ncol(d.training))
#a = d.training
plot(a[,3], type = 'l')
lines(d.training[,3], col = 'red')
d.training = a
rate= c(30, .8, 60, 10, 10, 10, 2, 35)
d.training = d.training + matrix(rexp(length(d.training), rate[model]),
nrow(d.training), ncol(d.training))
#a = d.training
plot(a[,3], type = 'l')
lines(d.training[,3], col = 'red')
d.training = a
#a = d.training
plot(a[,1], type = 'l')
lines(d.training[,1], col = 'red')
d.training = a
rate= c(30, .8, 60, 10, 10, 10, 2, 35)
d.training = d.training + matrix(rexp(length(d.training), rate[model]),
nrow(d.training), ncol(d.training))
#a = d.training
plot(a[,1], type = 'l')
lines(d.training[,1], col = 'red')
d.training = a
source('~/Desktop/ExtensiveAnalysis/main.r')
source('~/Desktop/ExtensiveAnalysis/main.r')
plot(rho$true.trace, type = 'l', lwd = 3)
lines(rho$inferred.trace, col = 'red')
plot(scale(rho$true.trace), type = 'l', lwd = 3)
lines(scale(rho$inferred.trace), col = 'red')
jacobiano_analitico <- function(nome, Inferred, num_species, origin, end){
### Inferred is the infer jacobian coefficients
### example jacobiano_analitico(BestCoefficients.elnet$J, 2, t.min, t.min+length.training-2)
nomi = list()
nomi$row = nomi$col = LETTERS[1:num_species]
all.data = as.matrix(read.table(nome, header = F))
### Nota importante: e fondamentale che transponi la matrice per come li stampi da python
J = lapply(origin:end, function(x, X) t(matrix(X[x,], num_species,
num_species, dimnames = nomi)), all.data)
result = Models.coefficient.correlation(Inferred, J, dfdx)
tr.true = unlist(lapply(2:(length(J)), function(x, X) sum(diag(X[[x]])), J))
tr.inferred = unlist(lapply(1:length(Inferred), function(x, X) sum(diag(X[[x]])), Inferred))
tr.inferred = tr.inferred[!is.na(tr.inferred)]
trace.corr = cor(scale(tr.true), scale(tr.inferred))
return(list(correlation.matrix = result$mat.cor, rmse.matrix = result$mat.rmse,
analytical.jacobian = J, trace.correlation = trace.corr,
true.trace = tr.true, inferred.trace = tr.inferred))
}
Models.coefficient.correlation <- function(X, Y, dfdx){
### input X,Y = Best series of jacobian coefficients
###       dfdx = a dataframe with all the combination of names of species (Jacobian entry)
### output: A matrix of which each entry tells me the correlation coefficient between
###         matrix X and matrix Y
CorMat = rmse.mat = c()
for(k in 1:nrow(dfdx)){
X_ij = unlist(lapply(1:(length(X)), function(i, M) M[[i]][dfdx[k,1],dfdx[k,2]], X))
Y_ij = unlist(lapply(1:(length(Y)-1), function(i, M) M[[i]][dfdx[k,1],dfdx[k,2]], Y))
CorMat = c(CorMat, cor(X_ij, Y_ij))
rmse.mat = c(rmse.mat, sqrt(mean((X_ij - Y_ij)^2)))
}
return(list( mat.cor = matrix(CorMat, nrow(X[[1]]), nrow(X[[1]])),
mat.rmse = matrix(rmse.mat,nrow(X[[1]]), nrow(X[[1]]))))
}
###### Now check the quality of the inference
rho = jacobiano_analitico(JacobianName,
BestCoefficients$J,
ncol(d), t.min, t.min+length.training-1)
matrix.inference.rho = rho$correlation.matrix
####
cat('Trace inference:', rho$trace.correlation, '\n')
plot(scale(rho$true.trace), type = 'l', lwd = 3)
lines(scale(rho$inferred.trace), col = 'red')
source('~/Desktop/ExtensiveAnalysis/main.r')
plot(scale(rho$true.trace), type = 'l', lwd = 3)
lines(scale(rho$inferred.trace), col = 'red')
source('~/Desktop/ExtensiveAnalysis/main.r')
source('~/Desktop/ExtensiveAnalysis/main.r')
plot(scale(rho$true.trace), type = 'l', lwd = 3)
lines(scale(rho$inferred.trace), col = 'red')
source('~/Desktop/ExtensiveAnalysis/main.r')
hist(rexp(1000, 2))
hist(rexp(1000, 35))
hist(rexp(1000, .8))
rm(list = ls())
suppressMessages(library(Matrix))
suppressMessages(library(parallel))
suppressMessages(library(compiler))
suppressMessages(library(lars))
suppressMessages(library(elasticnet))
options(warn=-1)
#####################################################################################
source('Functions/Auxiliar.r')
source('Functions/elastic_net_fit.r')
source('Functions/LOOCV.r')
source('Functions/KernelFunctions.r')
source('Functions/OutOfSample.r')
source('Functions/Interactions.R')
source('Functions/TrainingError.r')
args = commandArgs(trailingOnly=TRUE)
if(length(args) == 0)
{
noise.type = 1
}else{
noise.type = as.numeric(args[[1]])
}
cat('Noise:', noise.type,'\n')
if(noise.type == 1)
{
folder.name = 'NoiseNo/'
}else if(noise.type == 2)
{
folder.name = 'NoiseNormal/'
}else if(noise.type == 3)
{
folder.name = 'NoiseGamma/'
} else if(noise.type == 4)
{
folder.name = 'NoiseExponential/'
}
options.models = c('inputFiles/deterministic_chaos_cr.txt',
'inputFiles/deterministic_chaos_fc.txt',
'inputFiles/deterministic_chaos_lv.txt',
'inputFiles/deterministic_chaos_k.txt',
'inputFiles/deterministic_chaos_a.txt',
'inputFiles/deterministic_chaos_d.txt',
'inputFiles/deterministic_chaos_ml.txt',
'inputFiles/deterministic_chaos_hs.txt')
options.jacobian = c('inputFiles/jacobian_chaos_cr.txt',
'inputFiles/jacobian_chaos_fc.txt',
'inputFiles/jacobian_chaos_lv.txt',
'inputFiles/jacobian_chaos_k.txt',
'inputFiles/jacobian_chaos_a.txt',
'inputFiles/jacobian_chaos_d.txt',
'inputFiles/jacobian_chaos_ml.txt',
'inputFiles/jacobian_chaos_hs.txt')
kernels = c(1,1,1,1,1,1,1,1)
ratios = c(1., 1,0.9,0.95,0.95,0.95,0.95,0.9)
training.size = c(400, 400,300,300,300,300,300,200)
min.lambda = c(-5, -5,-3, -3,-3,-3,-3, -2)
Kernel.Options = c('Exponential.Kernel', 'Epanechnikov.Kernel', 'TriCubic.Kernel')
if(noise.type == 1)
{
realizations = 1
}else{
realizations = 30
}
collect.statistics = matrix(0,realizations, 8)
#### Prepare for the fit in parallel (keep one core out)
Lavoratori = detectCores() - 1
cl <- makeCluster(Lavoratori, type = "FORK")
#### To Remove later on
if(noise.type == 3)
{
initial.model = 4
}else{
initial.model = 1
}
model = 4
choice = model
kernel.choice = kernels[choice]
alpha = ratios[choice]
####
RegressionType = 'ELNET_fit'
###################################################################
FileName = options.models[choice]
JacobianName = options.jacobian[choice]
####################################
Regression.Kernel = Kernel.Options[kernel.choice]
###################################
#cat('Model:',  FileName, '\nKernel:', Regression.Kernel,'\n')
###################################
logspace <- function(d1, d2, n) exp(log(10)*seq(d1, d2, length.out=n))
std_err <- function(x) sd(x)/sqrt(length(x))
############# Parameters for cross validation
lambda = logspace(min.lambda[choice],0,15)
tht = logspace(-1.5,1.2,15)
parameters_on_grid = expand.grid(tht, lambda)
##### length of training and test set
length.training = training.size[choice]
length.testing = 30
### Read Time series
d = ReadTimeSeries(FileName)
### Use all the species
Embedding = LETTERS[1:ncol(d)]
TargetList = Embedding
######################
dfdx = expand.grid(TargetList, TargetList)
######################
d = d[, Embedding]
result = c()
t.min = 1
#### If you want to measure performance across random starting points initialized randomly
#t.min = floor(runif(1,1, nrow(d) - length.training - length.testing-1))
#### Random Chunk of length:
length_of_interval = length.training + length.testing
t.max = t.min + length_of_interval - 1
interval = t.min:t.max
interval_training = 1:length.training
interval_testing = (length.training + 1):length_of_interval
#### Subset the chunk
d_intact = d[interval,]
d.training = d_intact[interval_training, ]
#### Make noise if you want
ObservationalNoise = c('no_noise', 'normal', 'gamma', 'exponential')
ObservationalNoise = ObservationalNoise[noise.type]
a = d.training
plot(a[,1], type = 'l')
rate= c(30, .8, 60, 10, 10, 10, 2, 35)
d.training = d.training + matrix(rexp(length(d.training), rate[model]),
nrow(d.training), ncol(d.training))
lines(d.training[,1], col = 'red')
d.training = a
rumore = c(0.05,0.05,0.05,0.1,0.1,0.05,0.05,0.05)
d.training = d.training + matrix(rnorm(length(d.training),
0,sd(d.training)*rumore[model]),
nrow(d.training), ncol(d.training))
#a = d.training
plot(a[,1], type = 'l')
lines(d.training[,1], col = 'red')
#a = d.training
plot(a[,2], type = 'l')
lines(d.training[,2], col = 'red')
#a = d.training
plot(a[,3], type = 'l')
lines(d.training[,3], col = 'red')
d.training = a
rate= c(30, .8, 60, 10, 10, 10, 2, 35)
d.training = d.training + matrix(rexp(length(d.training), rate[model]),
nrow(d.training), ncol(d.training))
#a = d.training
plot(a[,3], type = 'l')
lines(d.training[,3], col = 'red')
d.training = a
shape = c(30, .8, 60, 10, 10, 10, 2, 35)
d.training = d.training + matrix(rgamma(length(d.training), 1, shape[model]),
nrow(d.training), ncol(d.training))
#a = d.training
plot(a[,3], type = 'l')
lines(d.training[,3], col = 'red')
d.training = a
shape = c(30, .8, 60, 15, 10, 10, 2, 35)
d.training = d.training + matrix(rgamma(length(d.training), 1, shape[model]),
nrow(d.training), ncol(d.training))
#a = d.training
plot(a[,3], type = 'l')
lines(d.training[,3], col = 'red')
d.training = a
rate= c(30, .8, 60, 15, 10, 10, 2, 35)
d.training = d.training + matrix(rexp(length(d.training), rate[model]),
nrow(d.training), ncol(d.training))
#a = d.training
plot(a[,3], type = 'l')
lines(d.training[,3], col = 'red')
model = 7
choice = model
kernel.choice = kernels[choice]
alpha = ratios[choice]
####
RegressionType = 'ELNET_fit'
###################################################################
FileName = options.models[choice]
JacobianName = options.jacobian[choice]
####################################
Regression.Kernel = Kernel.Options[kernel.choice]
###################################
#cat('Model:',  FileName, '\nKernel:', Regression.Kernel,'\n')
###################################
logspace <- function(d1, d2, n) exp(log(10)*seq(d1, d2, length.out=n))
std_err <- function(x) sd(x)/sqrt(length(x))
############# Parameters for cross validation
lambda = logspace(min.lambda[choice],0,15)
tht = logspace(-1.5,1.2,15)
parameters_on_grid = expand.grid(tht, lambda)
##### length of training and test set
length.training = training.size[choice]
length.testing = 30
### Read Time series
d = ReadTimeSeries(FileName)
### Use all the species
Embedding = LETTERS[1:ncol(d)]
TargetList = Embedding
######################
dfdx = expand.grid(TargetList, TargetList)
######################
d = d[, Embedding]
result = c()
t.min = 1
#### If you want to measure performance across random starting points initialized randomly
#t.min = floor(runif(1,1, nrow(d) - length.training - length.testing-1))
#### Random Chunk of length:
length_of_interval = length.training + length.testing
t.max = t.min + length_of_interval - 1
interval = t.min:t.max
interval_training = 1:length.training
interval_testing = (length.training + 1):length_of_interval
#### Subset the chunk
d_intact = d[interval,]
d.training = d_intact[interval_training, ]
#### Make noise if you want
ObservationalNoise = c('no_noise', 'normal', 'gamma', 'exponential')
ObservationalNoise = ObservationalNoise[noise.type]
#a = d.training
plot(a[,3], type = 'l')
a = d.training
#a = d.training
plot(a[,3], type = 'l')
rumore = c(0.05,0.05,0.05,0.1,0.1,0.05,0.05,0.05)
d.training = d.training + matrix(rnorm(length(d.training),
0,sd(d.training)*rumore[model]),
nrow(d.training), ncol(d.training))
lines(d.training[,3], col = 'red')
d.training = a
shape = c(35, .8, 60, 15, 10, 10, 4, 35)
d.training = d.training + matrix(rgamma(length(d.training), 1, shape[model]),
nrow(d.training), ncol(d.training))
#a = d.training
plot(a[,3], type = 'l')
lines(d.training[,3], col = 'red')
d.training = a
rate= c(35, .8, 60, 15, 10, 10, 4, 35)
d.training = d.training + matrix(rexp(length(d.training), rate[model]),
nrow(d.training), ncol(d.training))
#a = d.training
plot(a[,3], type = 'l')
lines(d.training[,3], col = 'red')
d.training = a
source('~/Desktop/loo/main.r')
View(rho)
plot(scale(rho$true.trace), type = 'l', lwd = 3)
lines(scale(rho$inferred.trace), lwd = 3, col = 'red')
source('~/Desktop/loo/main.r')
out.of.samp = forecast.function(BestCoefficients,
BestParameters$BestTH,
BestParameters$BestLM,
alpha,
d.training, length.testing, Regression.Kernel)
source('Functions/OutOfSample2.r')
out.of.samp = forecast.function(BestCoefficients,
BestParameters$BestTH,
BestParameters$BestLM,
alpha,
d.training, length.testing, Regression.Kernel)
prd = out.of.samp$out_of_samp
d.testing =  d[(t.min + length.training):(t.min + length.training + length.testing - 1), ]
d.testing = Standardizza.test(d.testing, d.train.to.test.set)
cor(prd[,1],d.testing[,1])
cor(prd[,2],d.testing[,2])
correlation = mean(unlist(lapply(1:ncol(d.testing), function(x, X, Y) cor(X[,x], Y[,y]), prd, d.testing)))
correlation = mean(unlist(lapply(1:ncol(d.testing), function(x, X, Y) cor(X[,x], Y[,x]), prd, d.testing)))
cat('correlation:', correlation, '\n')
idx = 1
plot(d.testing[,idx], type = 'l', lwd = 3)
lines(prd[,idx], col = 'red', lwd = 3)
idx = 2
plot(d.testing[,idx], type = 'l', lwd = 3)
lines(prd[,idx], col = 'red', lwd = 3)
idx = 3
plot(d.testing[,idx], type = 'l', lwd = 3)
lines(prd[,idx], col = 'red', lwd = 3)
idx = 4
plot(d.testing[,idx], type = 'l', lwd = 3)
lines(prd[,idx], col = 'red', lwd = 3)
length.testing = 100
out.of.samp = forecast.function(BestCoefficients,
BestParameters$BestTH,
BestParameters$BestLM,
alpha,
d.training, length.testing, Regression.Kernel)
prd = out.of.samp$out_of_samp
d.testing =  d[(t.min + length.training):(t.min + length.training + length.testing - 1), ]
d.testing = Standardizza.test(d.testing, d.train.to.test.set)
correlation = mean(unlist(lapply(1:ncol(d.testing), function(x, X, Y) cor(X[,x], Y[,x]), prd, d.testing)))
cat('correlation:', correlation, '\n')
idx = 4
plot(d.testing[,idx], type = 'l', lwd = 3)
lines(prd[,idx], col = 'red', lwd = 3)
idx = 2
plot(d.testing[,idx], type = 'l', lwd = 3)
lines(prd[,idx], col = 'red', lwd = 3)
idx = 1
plot(d.testing[,idx], type = 'l', lwd = 3)
lines(prd[,idx], col = 'red', lwd = 3)
idx = 3
plot(d.testing[,idx], type = 'l', lwd = 3)
lines(prd[,idx], col = 'red', lwd = 3)
source('~/Desktop/loo/main.r')
source('~/Dropbox (MIT)/Smap/RVersion_loocv/main.r')
JacobianName = 'inputFiles/jacobian_chaos_k.txt'
read.table(JacobianName)
###### Now check the quality of the inference
rho = jacobiano_analitico(JacobianName,
BestCoefficients$J,
ncol(d), t.min, t.min+length.training-1)
matrix.inference.rho = rho$correlation.matrix
####
cat('Trace inference:', rho$trace.correlation, '\n')
plot(scale(rho$true.trace), type = 'l', lwd = 3)
lines(scale(rho$inferred.trace), lwd = 3, col = 'red')
#### Forecast
out.of.samp = forecast.function(BestCoefficients,
BestParameters$BestTH,
BestParameters$BestLM,
alpha,
d.training, length.testing, Regression.Kernel)
prd = out.of.samp$out_of_samp
d.testing =  d[(t.min + length.training):(t.min + length.training + length.testing - 1), ]
d.testing = Standardizza.test(d.testing, d.train.to.test.set)
correlation = mean(unlist(lapply(1:ncol(d.testing), function(x, X, Y) cor(X[,x], Y[,x]), prd, d.testing)))
cat('correlation:', correlation, '\n')
idx = 3
plot(d.testing[,idx], type = 'l', lwd = 3)
lines(prd[,idx], col = 'red', lwd = 3)
idx = 2
plot(d.testing[,idx], type = 'l', lwd = 3)
lines(prd[,idx], col = 'red', lwd = 3)
length.testing = 100
#### Forecast
out.of.samp = forecast.function(BestCoefficients,
BestParameters$BestTH,
BestParameters$BestLM,
alpha,
d.training, length.testing, Regression.Kernel)
prd = out.of.samp$out_of_samp
d.testing =  d[(t.min + length.training):(t.min + length.training + length.testing - 1), ]
d.testing = Standardizza.test(d.testing, d.train.to.test.set)
correlation = mean(unlist(lapply(1:ncol(d.testing), function(x, X, Y) cor(X[,x], Y[,x]), prd, d.testing)))
cat('correlation:', correlation, '\n')
idx = 2
plot(d.testing[,idx], type = 'l', lwd = 3)
lines(prd[,idx], col = 'red', lwd = 3)
source('~/Dropbox (MIT)/RVersion/loocv/main.r')
source('~/Dropbox (MIT)/RVersion/loocv/main.r')
source('~/Dropbox (MIT)/RVersion/loocv/main.r')
###### Now check the quality of the inference
rho = jacobiano_analitico(JacobianName,
BestCoefficients$J,
ncol(d), t.min, t.min+length.training-1)
matrix.inference.rho = rho$correlation.matrix
####
cat('Trace inference:', rho$trace.correlation, '\n')
plot(scale(rho$true.trace), type = 'l', lwd = 3)
lines(scale(rho$inferred.trace), lwd = 3, col = 'red')
#### Forecast
out.of.samp = forecast.function(BestCoefficients,
BestParameters$BestTH,
BestParameters$BestLM,
alpha,
d.training, length.testing, Regression.Kernel)
prd = out.of.samp$out_of_samp
d.testing =  d[(t.min + length.training):(t.min + length.training + length.testing - 1), ]
d.testing = Standardizza.test(d.testing, d.train.to.test.set)
correlation = mean(unlist(lapply(1:ncol(d.testing), function(x, X, Y) cor(X[,x], Y[,x]), prd, d.testing)))
cat('correlation:', correlation, '\n')
idx = 2
plot(d.testing[,idx], type = 'l', lwd = 3)
lines(prd[,idx], col = 'red', lwd = 3)
idx = 4
plot(d.testing[,idx], type = 'l', lwd = 3)
lines(prd[,idx], col = 'red', lwd = 3)
idx = 1
plot(d.testing[,idx], type = 'l', lwd = 3)
lines(prd[,idx], col = 'red', lwd = 3)
source('~/Dropbox (MIT)/RVersion/loocv/main.r')
source('~/GitHubRepo/RegularizedSmap/RVersion/loocv/main.r')
